{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoModelForMaskedLM\n\nmodel_checkpoint = \"distilbert-base-uncased\"\nmodel = AutoModelForMaskedLM.from_pretrained(model_checkpoint)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:47:43.113427Z","iopub.execute_input":"2025-06-23T14:47:43.113662Z","iopub.status.idle":"2025-06-23T14:48:13.106496Z","shell.execute_reply.started":"2025-06-23T14:47:43.113644Z","shell.execute_reply":"2025-06-23T14:48:13.105975Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4fa94ed2e0d4427bd2509a4bf5a616e"}},"metadata":{}},{"name":"stderr","text":"2025-06-23 14:47:58.394357: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750690078.605294      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750690078.661664      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0637865fb3ad432aa9c01cd5b77a853d"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:48:13.108141Z","iopub.execute_input":"2025-06-23T14:48:13.108538Z","iopub.status.idle":"2025-06-23T14:48:14.232211Z","shell.execute_reply.started":"2025-06-23T14:48:13.108521Z","shell.execute_reply":"2025-06-23T14:48:14.231461Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcdca207dfcc472aba8a5302818aa32f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8c796fdb71b42ab97928fc502b32279"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b35dceeaf7442cd980d261e124ebec7"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\n\nimdb_dataset = load_dataset(\"imdb\")\nimdb_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:48:14.232749Z","iopub.execute_input":"2025-06-23T14:48:14.233116Z","iopub.status.idle":"2025-06-23T14:48:20.666211Z","shell.execute_reply.started":"2025-06-23T14:48:14.233096Z","shell.execute_reply":"2025-06-23T14:48:20.665615Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18339126ad4f4b24b1febc51144bbc65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"229dcb135ffb4c13b1bc2231ce2258c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa01fd7764c9413ca71520e6f00a52c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f00d85fe2588401e85e073dc9f688a8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e6dfc2d2c5d41c6b23bde0dd6212133"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b1d9069935a4011a067585db5872baf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b97451d01c3847ce9546395314814a56"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# For both auto-regressive and masked language modeling, a common preprocessing step is to concatenate all the examples and then split the whole corpus into chunks of equal size. This is quite different from our usual approach, where we simply tokenize individual examples. Why concatenate everything together? The reason is that individual examples might get truncated if they’re too long, and that would result in losing information that might be useful for the language modeling task","metadata":{}},{"cell_type":"markdown","source":"So I will first tokanize the whole data then group them all together and split the result into chunks","metadata":{}},{"cell_type":"code","source":"def tokenize_function(examples):\n    result = tokenizer(examples[\"text\"])\n    if tokenizer.is_fast:\n        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n    return result\n\n\n# Use batched=True to activate fast multithreading!\ntokenized_datasets = imdb_dataset.map(\n    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n)\ntokenized_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:48:20.666848Z","iopub.execute_input":"2025-06-23T14:48:20.667370Z","iopub.status.idle":"2025-06-23T14:49:11.849958Z","shell.execute_reply.started":"2025-06-23T14:48:20.667349Z","shell.execute_reply":"2025-06-23T14:49:11.849352Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02847c0afe524e898c0705ed6b2bfba3"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab865de201674a27aeeb9a793f78413b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99d95afb78c643aba5805e19a98f479a"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids'],\n        num_rows: 50000\n    })\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"So the best thing to know the how big the chunks is to see the model context size","metadata":{}},{"cell_type":"code","source":"tokenizer.model_max_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:49:11.850664Z","iopub.execute_input":"2025-06-23T14:49:11.850912Z","iopub.status.idle":"2025-06-23T14:49:11.855638Z","shell.execute_reply.started":"2025-06-23T14:49:11.850893Z","shell.execute_reply":"2025-06-23T14:49:11.854982Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"512"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"chunk_size = 128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:49:11.856389Z","iopub.execute_input":"2025-06-23T14:49:11.857057Z","iopub.status.idle":"2025-06-23T14:49:12.612296Z","shell.execute_reply.started":"2025-06-23T14:49:11.857038Z","shell.execute_reply":"2025-06-23T14:49:12.611638Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Let's see on small samples how I will group all the tokens together and then split them into eqaul chunks size","metadata":{}},{"cell_type":"code","source":"# Slicing produces a list of lists for each feature\ntokenized_samples = tokenized_datasets[\"train\"][:3]\n\nfor idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n    print(f\"'>>> Review {idx} length: {len(sample)}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:49:12.614494Z","iopub.execute_input":"2025-06-23T14:49:12.614700Z","iopub.status.idle":"2025-06-23T14:49:12.628422Z","shell.execute_reply.started":"2025-06-23T14:49:12.614682Z","shell.execute_reply":"2025-06-23T14:49:12.627546Z"}},"outputs":[{"name":"stdout","text":"'>>> Review 0 length: 363'\n'>>> Review 1 length: 304'\n'>>> Review 2 length: 133'\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"concatenated_examples = {\n    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n}\ntotal_length = len(concatenated_examples[\"input_ids\"])\nprint(f\"'>>> Concatenated reviews length: {total_length}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:49:12.629128Z","iopub.execute_input":"2025-06-23T14:49:12.629349Z","iopub.status.idle":"2025-06-23T14:49:12.643058Z","shell.execute_reply.started":"2025-06-23T14:49:12.629322Z","shell.execute_reply":"2025-06-23T14:49:12.642446Z"}},"outputs":[{"name":"stdout","text":"'>>> Concatenated reviews length: 800'\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"chunks = {\n    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n    for k, t in concatenated_examples.items()\n}\n\nfor chunk in chunks[\"input_ids\"]:\n    print(f\"'>>> Chunk length: {len(chunk)}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:49:12.643788Z","iopub.execute_input":"2025-06-23T14:49:12.644131Z","iopub.status.idle":"2025-06-23T14:49:12.658041Z","shell.execute_reply.started":"2025-06-23T14:49:12.644112Z","shell.execute_reply":"2025-06-23T14:49:12.657341Z"}},"outputs":[{"name":"stdout","text":"'>>> Chunk length: 128'\n'>>> Chunk length: 128'\n'>>> Chunk length: 128'\n'>>> Chunk length: 128'\n'>>> Chunk length: 128'\n'>>> Chunk length: 128'\n'>>> Chunk length: 32'\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"As we see the last chunk has size less than the others so we have two approaches:\n\n1- Drop the last chunk\n\n2- Pad the last chunk until its length equals chunk_size.","metadata":{}},{"cell_type":"markdown","source":"# Here I will use the first approach AND make the chunks process to the whole dataset","metadata":{}},{"cell_type":"code","source":"def group_texts(examples):\n    # Concatenate all texts\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    # Compute length of concatenated texts\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    # We drop the last chunk if it's smaller than chunk_size\n    total_length = (total_length // chunk_size) * chunk_size\n    # Split by chunks of max_len\n    result = {\n        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n        for k, t in concatenated_examples.items()\n    }\n    # Create a new labels column that equal to input_ids bec our goal is to predict the masked word\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:49:12.658832Z","iopub.execute_input":"2025-06-23T14:49:12.659701Z","iopub.status.idle":"2025-06-23T14:49:12.671487Z","shell.execute_reply.started":"2025-06-23T14:49:12.659677Z","shell.execute_reply":"2025-06-23T14:49:12.670862Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"lm_datasets = tokenized_datasets.map(group_texts, batched=True)\nlm_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:49:12.672167Z","iopub.execute_input":"2025-06-23T14:49:12.672383Z","iopub.status.idle":"2025-06-23T14:52:36.953375Z","shell.execute_reply.started":"2025-06-23T14:49:12.672368Z","shell.execute_reply":"2025-06-23T14:52:36.952812Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88889c1445cb4cf8961eea36cce3375a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1703c67182bb49b3af8a1e776745685e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"390e20ec00fc4b529db54d7ecf89beb6"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 61291\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 59904\n    })\n    unsupervised: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 122957\n    })\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# Next step is to insert randomly [MASK] tokens to make the model train and predict on it","metadata":{}},{"cell_type":"markdown","source":"I will use the datacollator for Language modeling that have a param name mlm_prob which using to know how many tokens will be [MASK] in each input senetence","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:52:36.954083Z","iopub.execute_input":"2025-06-23T14:52:36.954295Z","iopub.status.idle":"2025-06-23T14:52:37.006067Z","shell.execute_reply.started":"2025-06-23T14:52:36.954274Z","shell.execute_reply":"2025-06-23T14:52:37.005365Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"samples = [lm_datasets[\"train\"][i] for i in range(2)]\nfor sample in samples:\n    _ = sample.pop(\"word_ids\")\n\nfor chunk in data_collator(samples)[\"input_ids\"]:\n    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:52:37.006921Z","iopub.execute_input":"2025-06-23T14:52:37.007197Z","iopub.status.idle":"2025-06-23T14:52:37.034189Z","shell.execute_reply.started":"2025-06-23T14:52:37.007172Z","shell.execute_reply":"2025-06-23T14:52:37.033613Z"}},"outputs":[{"name":"stdout","text":"\n'>>> [CLS] i rented i am curious - [MASK] from my video [MASK] because of all the controversy that surrounded it when it昭 first released in 1967. i also heard that at first it was seized by u. s. [MASK] [MASK] it ever tried to 296 this country, therefore being [MASK] fan of films considered \" controversial \" i [MASK] had to see this [MASK] myself. < br / > < br / > the plot is centered around a [MASK] swedish drama student named lena who [MASK] to learn everything [MASK] can [MASK] life [MASK] in particular she wants [MASK] [MASK] her attentions to [MASK] [MASK] [MASK] of documentary on what the average swede [MASK] about certain political [MASK] such'\n\n'>>> as the vietnam war and race [MASK] in the [MASK] states. in between asking politicians and ordinary denizens [MASK] stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men [MASK] < br / pouring < br / > what kills [MASK] about i am curious - yellow [MASK] that 40 years ago, this was considered pornographic. really [MASK] the sex and nudity scenes are few [MASK] far between, even then it ' [MASK] not shot like some cheaply [MASK] [MASK]o. while my country [MASK] mind find it shocking, in reality sex and nu [MASK] are [MASK] major staple [MASK] swedish cinema. even ing [MASK] bergman,'\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# but I will face a problem with the standard masking probabilty because if there is a word \"quick\" and after tokenization split into two word :qu\" ,\"##ick\" so the standarm MLM will mask on word of them and leave the other which will be a problem","metadata":{}},{"cell_type":"markdown","source":"# So the solution is to apply a datacollator that apply masking for the whole word tokens( masks all subword tokens of a selected word at once)","metadata":{}},{"cell_type":"code","source":"import collections\nimport numpy as np\n\nfrom transformers import default_data_collator\n\nwwm_probability = 0.2\n\n\ndef whole_word_masking_data_collator(features):\n    for feature in features:\n        word_ids = feature.pop(\"word_ids\")\n\n        # Create a map between words and corresponding token indices\n        mapping = collections.defaultdict(list)\n        current_word_index = -1\n        current_word = None\n        for idx, word_id in enumerate(word_ids):\n            if word_id is not None:\n                if word_id != current_word:\n                    current_word = word_id\n                    current_word_index += 1\n                mapping[current_word_index].append(idx)\n\n        # Randomly mask words\n        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n        input_ids = feature[\"input_ids\"]\n        labels = feature[\"labels\"]\n        new_labels = [-100] * len(labels)\n        for word_id in np.where(mask)[0]:\n            word_id = word_id.item()\n            for idx in mapping[word_id]:\n                new_labels[idx] = labels[idx]\n                input_ids[idx] = tokenizer.mask_token_id\n        feature[\"labels\"] = new_labels\n\n    return default_data_collator(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:52:37.034850Z","iopub.execute_input":"2025-06-23T14:52:37.035095Z","iopub.status.idle":"2025-06-23T14:52:37.040805Z","shell.execute_reply.started":"2025-06-23T14:52:37.035074Z","shell.execute_reply":"2025-06-23T14:52:37.040262Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"samples = [lm_datasets[\"train\"][i] for i in range(2)]\nbatch = whole_word_masking_data_collator(samples)\n\nfor chunk in batch[\"input_ids\"]:\n    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:52:37.041521Z","iopub.execute_input":"2025-06-23T14:52:37.041777Z","iopub.status.idle":"2025-06-23T14:52:37.060482Z","shell.execute_reply.started":"2025-06-23T14:52:37.041762Z","shell.execute_reply":"2025-06-23T14:52:37.059949Z"}},"outputs":[{"name":"stdout","text":"\n'>>> [CLS] i rented i [MASK] [MASK] - yellow from my video store because of all the controversy that [MASK] [MASK] when it was first released in 1967 [MASK] i also [MASK] that at first it was seized by u [MASK] s. customs if it ever tried to enter this [MASK], therefore being [MASK] fan of [MASK] considered \" [MASK] \" i really had to see [MASK] for myself. < br / > [MASK] br / > the plot is [MASK] around a young [MASK] drama student [MASK] lena who wants [MASK] learn everything she [MASK] about life. [MASK] particular [MASK] wants to focus her attentions to making [MASK] sort of documentary on what the [MASK] [MASK] [MASK] thought about [MASK] [MASK] [MASK] [MASK]'\n\n'>>> [MASK] the vietnam war and [MASK] issues in the united states. in [MASK] [MASK] [MASK] and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and [MASK] men. < br / > [MASK] [MASK] / > [MASK] kills [MASK] about i [MASK] curious - yellow is that [MASK] years ago, this [MASK] [MASK] pornographic. really, [MASK] sex [MASK] nudity scenes are few and far between [MASK] even then it ' [MASK] not [MASK] [MASK] some [MASK] [MASK] made [MASK] [MASK]. [MASK] my [MASK] [MASK] [MASK] [MASK] [MASK] shocking, in reality sex and nudity are a major staple in swedish cinema. [MASK] ingmar bergman,'\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"here I will reduce the size of data(downsampling)","metadata":{}},{"cell_type":"code","source":"train_size = 10_000\ntest_size = int(0.1 * train_size)\n\ndownsampled_dataset = lm_datasets[\"train\"].train_test_split(\n    train_size=train_size, test_size=test_size, seed=42\n)\ndownsampled_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:55:38.387066Z","iopub.execute_input":"2025-06-23T14:55:38.387318Z","iopub.status.idle":"2025-06-23T14:55:38.404840Z","shell.execute_reply.started":"2025-06-23T14:55:38.387300Z","shell.execute_reply":"2025-06-23T14:55:38.404292Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 10000\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nbatch_size = 64\n# Show the training loss with every epoch\nlogging_steps = len(downsampled_dataset[\"train\"]) // batch_size\nmodel_name = model_checkpoint.split(\"/\")[-1]\n\ntraining_args = TrainingArguments(\n    eval_strategy=\"epoch\",\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    fp16=True,\n    logging_steps=logging_steps,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:55:48.401442Z","iopub.execute_input":"2025-06-23T14:55:48.402040Z","iopub.status.idle":"2025-06-23T14:55:49.944571Z","shell.execute_reply.started":"2025-06-23T14:55:48.402006Z","shell.execute_reply":"2025-06-23T14:55:49.944054Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=downsampled_dataset[\"train\"],\n    eval_dataset=downsampled_dataset[\"test\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T15:00:49.090392Z","iopub.execute_input":"2025-06-23T15:00:49.090666Z","iopub.status.idle":"2025-06-23T15:00:49.094179Z","shell.execute_reply.started":"2025-06-23T15:00:49.090647Z","shell.execute_reply":"2025-06-23T15:00:49.093469Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# **perplexity** metric: it is the exponentional of cross entropy that used in language models as the labels is not know but how it workds?\n# by calculate the probabilities it assigns to the next word in all the sentences of the test set. High probabilities indicates that the model is not “surprised” or “perplexed” by the unseen examples, and suggests it has learned the basic patterns of grammar in the language\n\nsmall values means a good model","metadata":{}},{"cell_type":"markdown","source":"# Let's use Accelrate to speed up our training process","metadata":{}},{"cell_type":"code","source":"def insert_random_mask(batch):\n    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n    masked_inputs = data_collator(features)\n    # Create a new \"masked\" column for each column in the dataset\n    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T15:01:31.793357Z","iopub.execute_input":"2025-06-23T15:01:31.794095Z","iopub.status.idle":"2025-06-23T15:01:31.798405Z","shell.execute_reply.started":"2025-06-23T15:01:31.794064Z","shell.execute_reply":"2025-06-23T15:01:31.797668Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"downsampled_dataset = downsampled_dataset.remove_columns([\"word_ids\"])\neval_dataset = downsampled_dataset[\"test\"].map(\n    insert_random_mask,\n    batched=True,\n    remove_columns=downsampled_dataset[\"test\"].column_names,\n)\neval_dataset = eval_dataset.rename_columns(\n    {\n        \"masked_input_ids\": \"input_ids\",\n        \"masked_attention_mask\": \"attention_mask\",\n        \"masked_labels\": \"labels\",\n    }\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T15:01:37.984759Z","iopub.execute_input":"2025-06-23T15:01:37.985038Z","iopub.status.idle":"2025-06-23T15:01:38.315801Z","shell.execute_reply.started":"2025-06-23T15:01:37.985019Z","shell.execute_reply":"2025-06-23T15:01:38.315070Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f866d9a6bc99427eabf4c942011217c6"}},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import default_data_collator\n\nbatch_size = 64\ntrain_dataloader = DataLoader(\n    downsampled_dataset[\"train\"],\n    shuffle=True,\n    batch_size=batch_size,\n    collate_fn=data_collator,\n)\neval_dataloader = DataLoader(\n    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T15:01:46.657341Z","iopub.execute_input":"2025-06-23T15:01:46.657607Z","iopub.status.idle":"2025-06-23T15:01:46.662007Z","shell.execute_reply.started":"2025-06-23T15:01:46.657587Z","shell.execute_reply":"2025-06-23T15:01:46.661393Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T15:01:54.373777Z","iopub.execute_input":"2025-06-23T15:01:54.374555Z","iopub.status.idle":"2025-06-23T15:01:54.553945Z","shell.execute_reply.started":"2025-06-23T15:01:54.374531Z","shell.execute_reply":"2025-06-23T15:01:54.553221Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"from torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(), lr=5e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T15:01:59.557444Z","iopub.execute_input":"2025-06-23T15:01:59.557964Z","iopub.status.idle":"2025-06-23T15:01:59.561798Z","shell.execute_reply.started":"2025-06-23T15:01:59.557941Z","shell.execute_reply":"2025-06-23T15:01:59.561061Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"from accelerate import Accelerator\n\naccelerator = Accelerator()\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T15:02:27.142051Z","iopub.execute_input":"2025-06-23T15:02:27.142310Z","iopub.status.idle":"2025-06-23T15:02:27.276554Z","shell.execute_reply.started":"2025-06-23T15:02:27.142292Z","shell.execute_reply":"2025-06-23T15:02:27.275754Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from transformers import get_scheduler\n\nnum_train_epochs = 3\nnum_update_steps_per_epoch = len(train_dataloader)\nnum_training_steps = num_train_epochs * num_update_steps_per_epoch\n\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T15:02:32.742089Z","iopub.execute_input":"2025-06-23T15:02:32.742354Z","iopub.status.idle":"2025-06-23T15:02:32.746624Z","shell.execute_reply.started":"2025-06-23T15:02:32.742335Z","shell.execute_reply":"2025-06-23T15:02:32.745873Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport torch\nimport math\n\nprogress_bar = tqdm(range(num_training_steps))\n\nfor epoch in range(num_train_epochs):\n    # Training\n    model.train()\n    for batch in train_dataloader:\n        outputs = model(**batch)\n        loss = outputs.loss\n        accelerator.backward(loss)\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n\n    # Evaluation\n    model.eval()\n    losses = []\n    for step, batch in enumerate(eval_dataloader):\n        with torch.no_grad():\n            outputs = model(**batch)\n\n        loss = outputs.loss\n        losses.append(accelerator.gather(loss.repeat(batch_size)))\n\n    losses = torch.cat(losses)\n    losses = losses[: len(eval_dataset)]\n    try:\n        perplexity = math.exp(torch.mean(losses))\n    except OverflowError:\n        perplexity = float(\"inf\")\n\n    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n\n    # Save and upload\n    accelerator.wait_for_everyone()\n    unwrapped_model = accelerator.unwrap_model(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T15:02:54.079850Z","iopub.execute_input":"2025-06-23T15:02:54.080535Z","iopub.status.idle":"2025-06-23T15:05:35.781031Z","shell.execute_reply.started":"2025-06-23T15:02:54.080512Z","shell.execute_reply":"2025-06-23T15:05:35.780263Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/471 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5f1e0ef9ad24be69c3d2e6f107a13fd"}},"metadata":{}},{"name":"stdout","text":">>> Epoch 0: Perplexity: 11.572237091425201\n>>> Epoch 1: Perplexity: 11.094611832088468\n>>> Epoch 2: Perplexity: 10.893418485699002\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}